---
phase: 03-single-persona-feedback
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - .lesson-designer/sessions/test-persona-flow/
autonomous: false

must_haves:
  truths:
    - "Complete persona feedback workflow executes end-to-end"
    - "Teacher can review and approve revision plan"
    - "Approved revisions are applied to lesson design"
    - "Architecture is proven ready for Phase 4 scaling"
  artifacts:
    - path: ".lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json"
      provides: "Persona feedback from evaluation"
      contains: "concerns"
    - path: ".lesson-designer/sessions/test-persona-flow/03_revision_plan.md"
      provides: "Teacher-readable revision plan"
      contains: "Teacher decision"
    - path: ".lesson-designer/sessions/test-persona-flow/04_lesson_final.json"
      provides: "Lesson design with approved revisions applied"
      contains: "activities"
  key_links:
    - from: "persona_evaluator.py"
      to: "04_lesson_final.json"
      via: "evaluates lesson design"
      pattern: "feedback"
    - from: "generate_revision_plan.py"
      to: "revision_plan.md"
      via: "renders teacher-readable plan"
      pattern: "markdown"
---

<objective>
Execute complete persona feedback workflow end-to-end and verify with human teacher review.

Purpose: Prove the Phase 3 feedback loop architecture works from lesson design through persona evaluation, revision plan generation, teacher approval, and revision application. This validates that the system is ready for Phase 4 scaling to 4 personas.

Output:
- Working feedback loop demonstrated with test lesson
- Teacher approval workflow verified
- Architecture proven scalable (parameterized, documented)
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-single-persona-feedback/03-RESEARCH.md

# From Plan 01
@.claude/skills/lesson-designer/personas/struggling_learner.json
@.claude/skills/lesson-designer/scripts/persona_evaluator.py

# From Plan 02
@.claude/skills/lesson-designer/scripts/generate_revision_plan.py
@.claude/skills/lesson-designer/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test lesson and run complete feedback workflow</name>
  <files>
    .lesson-designer/sessions/test-persona-flow/04_lesson_final.json
    .lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json
    .lesson-designer/sessions/test-persona-flow/03_revision_plan.json
    .lesson-designer/sessions/test-persona-flow/03_revision_plan.md
  </files>
  <action>
Create a realistic test lesson that will trigger persona feedback, then run the complete Stage 3.5 workflow.

**Step 1: Create test session directory and lesson**

Create `.lesson-designer/sessions/test-persona-flow/` with a lesson designed to trigger feedback:
- Include vocabulary terms WITHOUT definitions (should trigger vocabulary concern)
- Include 25+ min activity without break (should trigger pacing concern)
- Include writing task without sentence frames (should trigger scaffolding concern)
- Include some strengths (activity variety, visuals) so feedback is balanced

Example lesson structure:
```json
{
  "title": "Analyzing Primary Sources: Civil War Era",
  "grade_level": "8th grade",
  "duration": 50,
  "lesson_type": "introducing",
  "objective": "Students will analyze primary sources to evaluate historical claims",
  "vocabulary": [
    {"word": "bias", "definition": null},
    {"word": "reliability", "definition": null},
    {"word": "primary source", "definition": "An original document from the time period"},
    {"word": "perspective", "definition": null}
  ],
  "activities": [
    {
      "name": "Vocabulary Introduction",
      "duration": 5,
      "marzano_level": "retrieval",
      "instructions": ["Review vocabulary terms on board", "Students copy into notes"],
      "student_output": "notes",
      "materials": ["vocabulary list on board"]
    },
    {
      "name": "Document Analysis Practice",
      "duration": 25,
      "marzano_level": "analysis",
      "instructions": [
        "Read the Civil War letter carefully",
        "Complete the SOAP analysis worksheet",
        "Identify the speaker, occasion, audience, and purpose",
        "Write a 2-paragraph response explaining whether the source is reliable"
      ],
      "student_output": "written response",
      "materials": ["Civil War letter (image)", "SOAP worksheet"]
    },
    {
      "name": "Whole Class Discussion",
      "duration": 15,
      "marzano_level": "analysis",
      "instructions": ["Share findings with class", "Compare perspectives"],
      "student_output": "verbal responses",
      "materials": []
    },
    {
      "name": "Exit Ticket",
      "duration": 5,
      "marzano_level": "comprehension",
      "instructions": ["Answer the exit ticket question"],
      "student_output": "written response",
      "materials": ["exit ticket form"]
    }
  ],
  "assessment": {
    "type": "exit_ticket",
    "questions": ["Why is it important to evaluate source reliability?"]
  }
}
```

**Step 2: Run persona evaluation**

```bash
python .claude/skills/lesson-designer/scripts/persona_evaluator.py \
    .lesson-designer/sessions/test-persona-flow/04_lesson_final.json \
    .claude/skills/lesson-designer/personas/struggling_learner.json \
    .lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json
```

**Step 3: Generate revision plan**

```bash
python .claude/skills/lesson-designer/scripts/generate_revision_plan.py \
    .lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json \
    .lesson-designer/sessions/test-persona-flow/03_revision_plan.json \
    --markdown .lesson-designer/sessions/test-persona-flow/03_revision_plan.md
```

**Step 4: Verify outputs exist and are valid**

```bash
# Check feedback exists and has concerns
python -c "
import json
fb = json.load(open('.lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json'))
print('Accessibility rating:', fb['overall_assessment']['accessibility_rating'])
print('Concerns found:', len(fb['concerns']))
assert len(fb['concerns']) >= 2, 'Expected at least 2 concerns'
"

# Check revision plan exists with changes
python -c "
import json
plan = json.load(open('.lesson-designer/sessions/test-persona-flow/03_revision_plan.json'))
print('Critical changes:', len(plan['critical_changes']))
print('Optional improvements:', len(plan['optional_improvements']))
"

# Display revision plan markdown
cat .lesson-designer/sessions/test-persona-flow/03_revision_plan.md
```
  </action>
  <verify>
ls .lesson-designer/sessions/test-persona-flow/*.json && \
ls .lesson-designer/sessions/test-persona-flow/*.md && \
python -c "
import json
fb = json.load(open('.lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json'))
assert 'concerns' in fb and len(fb['concerns']) >= 2
plan = json.load(open('.lesson-designer/sessions/test-persona-flow/03_revision_plan.json'))
assert 'critical_changes' in plan
print('Workflow produced valid outputs')
"
  </verify>
  <done>
- Test session directory created with lesson design
- Persona evaluation ran and produced feedback JSON
- Revision plan generator produced JSON and Markdown
- Feedback identifies at least 2 concerns (vocabulary, scaffolding/pacing)
- Revision plan has critical changes and optional improvements
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete persona feedback workflow:
1. Struggling learner persona ("Alex") evaluated test lesson
2. Identified accessibility concerns with severity ratings
3. Generated teacher-readable revision plan with recommendations
4. System ready for teacher approval flow
  </what-built>
  <how-to-verify>
**Review the revision plan:**

1. Open `.lesson-designer/sessions/test-persona-flow/03_revision_plan.md`

2. Verify the feedback is pedagogically sound:
   - Does the struggling learner perspective make sense?
   - Are the concerns legitimate accessibility issues?
   - Are the recommendations actionable and specific?

3. Verify the format is teacher-friendly:
   - Is it easy to understand?
   - Are approve/reject options clear?
   - Is the rationale convincing?

4. Test the approval flow by responding with one of:
   - "approve all" - Apply all critical changes
   - "approve 1, reject 2" - Selective approval
   - "reject all" - Proceed with original lesson

**Expected output:**
- Markdown revision plan with 2+ critical changes
- Each change shows: element, issue, recommendation, rationale
- Teacher decision checkboxes for each change
  </how-to-verify>
  <resume-signal>Type "approved" after reviewing the revision plan, or describe any issues found.</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Apply approved revisions and verify updated lesson</name>
  <files>
    .lesson-designer/sessions/test-persona-flow/03_revision_plan.json
    .lesson-designer/sessions/test-persona-flow/04_lesson_final.json
  </files>
  <action>
After teacher approval, apply the approved revisions to the lesson design.

**Step 1: Update revision plan with approval status**

Update `03_revision_plan.json` to mark changes as approved:
```python
import json

# Load revision plan
plan = json.load(open('.lesson-designer/sessions/test-persona-flow/03_revision_plan.json'))

# Mark critical changes as approved (based on teacher response)
for change in plan['critical_changes']:
    change['status'] = 'approved'

# Save updated plan
json.dump(plan, open('.lesson-designer/sessions/test-persona-flow/03_revision_plan.json', 'w'), indent=2)
```

**Step 2: Apply revisions to lesson**

```python
import sys
sys.path.insert(0, '.claude/skills/lesson-designer/scripts')
from generate_revision_plan import apply_revisions

apply_revisions(
    lesson_path='.lesson-designer/sessions/test-persona-flow/04_lesson_final.json',
    revision_plan_path='.lesson-designer/sessions/test-persona-flow/03_revision_plan.json',
    output_path='.lesson-designer/sessions/test-persona-flow/04_lesson_final.json'
)
```

**Step 3: Verify revisions applied**

Check the updated lesson design:
- If vocabulary change approved: definitions should be added
- If scaffolding change approved: sentence_frames or worked_example flag should be added
- If pacing change approved: activity should have break_points or be split

```python
import json
lesson = json.load(open('.lesson-designer/sessions/test-persona-flow/04_lesson_final.json'))

# Check for vocabulary definitions
vocab_with_defs = [v for v in lesson.get('vocabulary', []) if v.get('definition')]
print(f'Vocabulary with definitions: {len(vocab_with_defs)}/{len(lesson.get("vocabulary", []))}')

# Check for scaffolding additions
for activity in lesson.get('activities', []):
    if activity.get('sentence_frames') or activity.get('include_worked_example'):
        print(f'Scaffolding added to: {activity["name"]}')
```
  </action>
  <verify>
python -c "
import json
lesson = json.load(open('.lesson-designer/sessions/test-persona-flow/04_lesson_final.json'))

# Check some revision was applied (vocabulary definitions or scaffolding)
has_revision = False

# Check vocabulary
for v in lesson.get('vocabulary', []):
    if v.get('definition') and v['word'] != 'primary source':  # primary source had definition originally
        has_revision = True
        print(f'Vocabulary revision applied: {v[\"word\"]} now has definition')

# Check scaffolding
for a in lesson.get('activities', []):
    if a.get('sentence_frames') or a.get('include_worked_example'):
        has_revision = True
        print(f'Scaffolding revision applied to: {a[\"name\"]}')

if has_revision:
    print('Revisions successfully applied')
else:
    print('Warning: No visible revisions found - check apply_revisions implementation')
"
  </verify>
  <done>
- Revision plan updated with approval status
- apply_revisions() executed successfully
- Lesson design updated with approved changes
- Changes visible in updated 04_lesson_final.json
  </done>
</task>

</tasks>

<verification>
Final verification of complete Phase 3 implementation:

```bash
# 1. Verify all files exist
echo "=== Files created ==="
ls -la .lesson-designer/sessions/test-persona-flow/

# 2. Verify persona definition is parameterized
echo ""
echo "=== Persona definition structure ==="
python -c "
import json
p = json.load(open('.claude/skills/lesson-designer/personas/struggling_learner.json'))
print('Persona:', p['persona_name'])
print('Evaluation criteria:', p['evaluation_criteria'])
print('Parameterized: Yes (works with any persona JSON)')
"

# 3. Verify SKILL.md has Stage 3.5
echo ""
echo "=== SKILL.md Stage 3.5 ==="
grep -A 3 "Stage 3.5" .claude/skills/lesson-designer/SKILL.md | head -10

# 4. Verify feedback loop completed
echo ""
echo "=== Feedback loop summary ==="
python -c "
import json
fb = json.load(open('.lesson-designer/sessions/test-persona-flow/03_feedback_struggling_learner.json'))
plan = json.load(open('.lesson-designer/sessions/test-persona-flow/03_revision_plan.json'))
lesson = json.load(open('.lesson-designer/sessions/test-persona-flow/04_lesson_final.json'))

print('Persona evaluation: Complete')
print(f'  - Accessibility rating: {fb[\"overall_assessment\"][\"accessibility_rating\"]}')
print(f'  - Concerns identified: {len(fb[\"concerns\"])}')

print('Revision plan: Complete')
print(f'  - Critical changes: {len(plan[\"critical_changes\"])}')
print(f'  - Optional improvements: {len(plan.get(\"optional_improvements\", []))}')

approved = [c for c in plan['critical_changes'] if c.get('status') == 'approved']
print(f'  - Changes approved: {len(approved)}/{len(plan[\"critical_changes\"])}')

print('Lesson revision: Complete')
print(f'  - Title: {lesson[\"title\"]}')
"

# 5. Architecture scalability check
echo ""
echo "=== Architecture scalability ==="
echo "Phase 4 readiness checklist:"
echo "  [x] Persona definitions are JSON configs (add more personas = add JSON files)"
echo "  [x] Evaluator is parameterized (works with any persona)"
echo "  [x] Feedback aggregation supports N personas"
echo "  [x] Revision plan can handle multiple feedback sources"
echo "  [x] Workflow documented in SKILL.md"
```
</verification>

<success_criteria>
- Complete feedback workflow executes: evaluate -> revise -> approve -> apply
- Teacher checkpoint allows review and approval of changes
- Revisions are applied to lesson design
- Architecture is proven scalable:
  - Parameterized persona evaluator
  - JSON-based persona definitions
  - Aggregation ready for N personas
  - Documented in SKILL.md
- Phase 3 requirements met:
  - PERS-01 (partial): Struggling learner persona runs
  - PERS-02: Persona provides reaction
  - PERS-03: Specific recommendations provided
  - PERS-04: Teacher confirms before finalizing
</success_criteria>

<output>
After completion, create `.planning/phases/03-single-persona-feedback/03-03-SUMMARY.md`
</output>
