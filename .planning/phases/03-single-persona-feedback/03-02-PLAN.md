---
phase: 03-single-persona-feedback
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - .claude/skills/lesson-designer/scripts/generate_revision_plan.py
  - .claude/skills/lesson-designer/SKILL.md
autonomous: true

must_haves:
  truths:
    - "Tool generates teacher-readable revision plan from persona feedback"
    - "Revision plan presents critical changes with approve/reject options"
    - "SKILL.md documents Stage 3.5 persona feedback workflow"
  artifacts:
    - path: ".claude/skills/lesson-designer/scripts/generate_revision_plan.py"
      provides: "Revision plan generator from persona feedback"
      exports: ["generate_revision_plan", "render_revision_markdown"]
    - path: ".claude/skills/lesson-designer/SKILL.md"
      provides: "Stage 3.5 workflow documentation"
      contains: "Stage 3.5"
  key_links:
    - from: "generate_revision_plan.py"
      to: "persona_evaluator.py"
      via: "reads feedback JSON output"
      pattern: "feedback.*json"
    - from: "SKILL.md"
      to: "Stage 3.5"
      via: "workflow stage documentation"
      pattern: "persona.*feedback"
---

<objective>
Create the revision plan generator and integrate persona feedback into the SKILL.md workflow as Stage 3.5.

Purpose: Transform structured persona feedback into a teacher-readable revision plan with clear approve/reject options. Document the complete feedback loop workflow so Claude can execute Stage 3.5 between lesson design validation (Stage 3b) and file generation (Stage 5).

Output:
- scripts/generate_revision_plan.py - Aggregates feedback, prioritizes changes, renders Markdown
- SKILL.md updated with Stage 3.5 documentation
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-single-persona-feedback/03-RESEARCH.md

# From Plan 01
@.claude/skills/lesson-designer/personas/struggling_learner.json
@.claude/skills/lesson-designer/scripts/persona_evaluator.py

# Current workflow
@.claude/skills/lesson-designer/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create revision plan generator script</name>
  <files>
    .claude/skills/lesson-designer/scripts/generate_revision_plan.py
  </files>
  <action>
Create generate_revision_plan.py that transforms persona feedback into a teacher-readable revision plan.

Script structure:
```python
#!/usr/bin/env python3
"""
Generate revision plan from persona feedback.

Usage:
    python generate_revision_plan.py <feedback_json> <output_json> [--markdown <output_md>]

Example:
    python generate_revision_plan.py \
        .lesson-designer/sessions/{session_id}/03_feedback_struggling_learner.json \
        .lesson-designer/sessions/{session_id}/03_revision_plan.json \
        --markdown .lesson-designer/sessions/{session_id}/03_revision_plan.md
"""

import json
import argparse
from datetime import datetime
from pathlib import Path

def aggregate_feedback(feedback_files):
    """Combine feedback from all personas (Phase 3: just 1)."""
    # Load feedback files
    # Extract all concerns
    # Return aggregated structure

def prioritize_revisions(concerns):
    """Organize by severity, group by element."""
    # Sort by severity (high > medium > low)
    # Group by element for coherent presentation
    # Return prioritized list

def generate_revision_plan(lesson_path, feedback_paths, output_path):
    """Create structured revision plan with specific changes."""
    # Load lesson and feedback
    # Aggregate concerns
    # Prioritize by severity
    # Structure into:
    #   - critical_changes (high severity)
    #   - optional_improvements (medium severity)
    #   - requires_teacher_decision (low severity or deferred items)
    # Include current_state, proposed_change, rationale, implementation_notes
    # Save revision plan JSON

def render_revision_markdown(revision_plan, lesson, output_md_path):
    """Generate teacher-readable Markdown from revision plan."""
    # Template structure:
    # # Revision Plan: {lesson_title}
    # **Lesson:** {title}
    # **Generated:** {date}
    # **Personas consulted:** {persona_names}
    #
    # ## Summary
    # {concern_count} concerns identified, {critical_count} critical
    #
    # ## Critical Changes (RECOMMENDED)
    # ### Change 1: {title}
    # **Element:** {element}
    # **Severity:** HIGH
    # **Persona:** {persona_name}
    # **Current State:** {description}
    # **Proposed Change:** {change}
    # **Rationale:** {rationale}
    # **Impact if not addressed:** {impact}
    # **Teacher decision:**
    # - [ ] Approve as written
    # - [ ] Approve with modifications: ___
    # - [ ] Reject (reason: ___)
    #
    # ## Optional Improvements (CONSIDER)
    # ### Change N: {title}
    # ...
    #
    # ## Requires Teacher Context (LOW PRIORITY)
    # ...
    #
    # ## Approval Summary
    # **Critical changes requiring approval:** {count}
    # **Estimated time impact:** +{minutes} minutes
```

Key functions:
- aggregate_feedback(): handles 1 persona now, N personas in Phase 4
- prioritize_revisions(): severity-based sorting
- generate_revision_plan(): creates JSON structure
- render_revision_markdown(): creates teacher-readable output
- apply_revisions(): applies approved changes to lesson JSON (for Phase 3 completion)

The revision plan JSON should include status fields for tracking teacher decisions:
- status: "pending" | "approved" | "approved_with_modifications" | "rejected"
- teacher_notes: str (optional modifications or rejection reason)

Include apply_revisions() function that reads approved revisions and updates lesson JSON.
  </action>
  <verify>
python -c "
import sys
sys.path.insert(0, '.claude/skills/lesson-designer/scripts')
from generate_revision_plan import aggregate_feedback, prioritize_revisions, generate_revision_plan, render_revision_markdown

# Create test feedback
feedback = {
    'persona': 'struggling_learner_ell',
    'persona_name': 'Alex',
    'concerns': [
        {'element': 'vocabulary', 'issue': 'Terms lack definitions', 'severity': 'high', 'recommendation': {'change': 'Add definitions', 'rationale': 'Vocab gaps'}},
        {'element': 'pacing', 'issue': 'Activity too long', 'severity': 'medium', 'recommendation': {'change': 'Add break', 'rationale': 'Attention span'}}
    ]
}
import json
import tempfile
with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
    json.dump(feedback, f)
    fb_path = f.name

agg = aggregate_feedback([fb_path])
prioritized = prioritize_revisions(agg['all_concerns'])
assert len(prioritized) == 2
assert prioritized[0]['severity'] == 'high'  # High severity first
print('Revision plan generator works')
"
  </verify>
  <done>
- generate_revision_plan.py exists with all key functions
- aggregate_feedback() combines feedback from persona(s)
- prioritize_revisions() sorts by severity
- generate_revision_plan() creates structured JSON
- render_revision_markdown() creates teacher-readable output
- apply_revisions() applies approved changes to lesson JSON
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Stage 3.5 to SKILL.md workflow</name>
  <files>
    .claude/skills/lesson-designer/SKILL.md
  </files>
  <action>
Update SKILL.md to document Stage 3.5: Persona Feedback & Revision.

Insert Stage 3.5 after Stage 3b (Validate Cognitive Rigor) and before Stage 5 (Generate Materials).

Add this section to SKILL.md:

```markdown
### Stage 3.5: Persona Feedback & Revision

**Purpose:** Evaluate lesson design through struggling learner persona and apply teacher-approved revisions before generating materials.

**Inputs:**
- Validated lesson design from Stage 3b (`04_lesson_final.json`)
- Persona definition (`.claude/skills/lesson-designer/personas/struggling_learner.json`)

**Process:**

#### Step 1: Run Persona Evaluation

```bash
python .claude/skills/lesson-designer/scripts/persona_evaluator.py \
    .lesson-designer/sessions/{session_id}/04_lesson_final.json \
    .claude/skills/lesson-designer/personas/struggling_learner.json \
    .lesson-designer/sessions/{session_id}/03_feedback_struggling_learner.json
```

This produces structured feedback JSON with:
- Accessibility rating (1-5 scale)
- Strengths (what works well for struggling learners)
- Concerns (issues with severity ratings)
- Recommendations (specific changes with rationale)

#### Step 2: Generate Revision Plan

```bash
python .claude/skills/lesson-designer/scripts/generate_revision_plan.py \
    .lesson-designer/sessions/{session_id}/03_feedback_struggling_learner.json \
    .lesson-designer/sessions/{session_id}/03_revision_plan.json \
    --markdown .lesson-designer/sessions/{session_id}/03_revision_plan.md
```

This produces:
- `03_revision_plan.json` - Structured revision plan
- `03_revision_plan.md` - Teacher-readable revision plan

#### Step 3: Present Revision Plan to Teacher

Present the Markdown revision plan to the teacher with this format:

```
Based on struggling learner feedback, I've identified {N} recommended changes to improve accessibility.

[Show 03_revision_plan.md content]

Do you approve the critical changes?
1. {Change 1 summary}
2. {Change 2 summary}

Reply "approve all" or specify which changes to approve/reject.
```

Wait for teacher response before proceeding.

#### Step 4: Apply Approved Revisions

If teacher approves changes:

```python
from generate_revision_plan import apply_revisions

apply_revisions(
    lesson_path='.lesson-designer/sessions/{session_id}/04_lesson_final.json',
    revision_plan_path='.lesson-designer/sessions/{session_id}/03_revision_plan.json',
    output_path='.lesson-designer/sessions/{session_id}/04_lesson_final.json'  # Overwrites with revised version
)
```

If teacher rejects all changes, proceed with original lesson design.

#### Step 5: Log Revision Decision

Update `03_revision_plan.json` with teacher decisions:
- status: "approved" | "approved_with_modifications" | "rejected"
- teacher_notes: Any modifications or rejection reasons

**Outputs:**
- `03_feedback_struggling_learner.json` - Persona feedback
- `03_revision_plan.json` - Revision plan with teacher decisions
- `03_revision_plan.md` - Teacher-readable plan
- `04_lesson_final.json` - Updated with approved revisions

**Evaluation Criteria:**

The struggling learner persona ("Alex") evaluates:
1. **Vocabulary Accessibility** - Tier 2/3 terms defined? Reading level appropriate?
2. **Instruction Clarity** - Steps numbered? <3 steps or checklist provided?
3. **Scaffolding Adequacy** - Models for writing tasks? Sentence frames? Graphic organizers?
4. **Pacing Appropriateness** - Activities <20 min? Break points for longer activities?
5. **Engagement Accessibility** - Multiple entry points? Choice opportunities?

**When to Skip Stage 3.5:**

Stage 3.5 can be skipped if:
- Teacher explicitly opts out (`"skip_persona_feedback": true` in 01_input.json)
- Lesson is specifically designed for advanced learners only

Default: Always run Stage 3.5 for inclusive lesson design.

**Requirements Covered:**
- PERS-01 (partial): Tool runs lesson through struggling learner persona
- PERS-02: Persona provides reaction describing likely response
- PERS-03: Persona provides specific pedagogical recommendations
- PERS-04: Tool proposes revisions; teacher confirms before finalizing

**Next:** Stage 5 (Generate Materials)
```

Also update:
1. Workflow Overview checklist - add Stage 3.5
2. Complete Workflow Checklist - add Stage 3.5 section
3. State Directory Structure - add new files (03_feedback_*, 03_revision_plan.*)
4. Quick Reference - add persona_evaluator.py and generate_revision_plan.py commands
  </action>
  <verify>
grep -q "Stage 3.5" .claude/skills/lesson-designer/SKILL.md && \
grep -q "persona_evaluator.py" .claude/skills/lesson-designer/SKILL.md && \
grep -q "generate_revision_plan.py" .claude/skills/lesson-designer/SKILL.md && \
echo "SKILL.md updated with Stage 3.5"
  </verify>
  <done>
- SKILL.md contains Stage 3.5: Persona Feedback & Revision section
- Workflow Overview checklist includes Stage 3.5
- Complete Workflow Checklist includes Stage 3.5 steps
- State Directory Structure shows new files (03_feedback_*, 03_revision_plan.*)
- Quick Reference includes new script commands
  </done>
</task>

</tasks>

<verification>
End-to-end test of revision plan generation:

```bash
# Create test feedback file
python -c "
import json
feedback = {
    'persona': 'struggling_learner_ell',
    'persona_name': 'Alex',
    'evaluation_date': '2026-01-26',
    'overall_assessment': {
        'accessibility_rating': '2/5',
        'summary': 'Lesson has barriers for struggling learners',
        'primary_concern': 'Vocabulary lacks definitions'
    },
    'strengths': [
        {'element': 'activity_variety', 'observation': 'Mix of activities', 'why_helpful': 'Multiple entry points'}
    ],
    'concerns': [
        {
            'element': 'vocabulary',
            'issue': '3 terms lack definitions (bias, reliability, perspective)',
            'severity': 'high',
            'impact': 'Students cannot understand core concepts',
            'evidence': 'Vocabulary list missing definitions',
            'recommendation': {
                'change': 'Add vocabulary pre-teaching section',
                'rationale': '96% of struggling readers have vocabulary gaps',
                'implementation': 'Create handout with term, definition, example, visual'
            }
        },
        {
            'element': 'scaffolding',
            'issue': 'Writing task without sentence frames',
            'severity': 'high',
            'impact': 'Students struggle with academic writing',
            'evidence': 'SOAP analysis requires writing without model',
            'recommendation': {
                'change': 'Add sentence frames',
                'rationale': 'Explicit models reduce cognitive load',
                'implementation': 'Provide frames like \"The speaker is ___ because ___\"'
            }
        },
        {
            'element': 'pacing',
            'issue': 'Activity runs 25 min without break',
            'severity': 'medium',
            'impact': 'Cognitive overload possible',
            'evidence': 'Document Analysis is 25 min continuous',
            'recommendation': {
                'change': 'Add 2-min turn-and-talk midpoint',
                'rationale': 'Struggling readers need breaks every 10-15 min',
                'implementation': 'Insert \"Share one thing you noticed\" at minute 12'
            }
        }
    ],
    'pedagogical_notes': {
        'priority_revisions': ['Add vocabulary pre-teaching', 'Add sentence frames'],
        'deferred_to_teacher': ['Primary source reading level - teacher has context']
    }
}
json.dump(feedback, open('test_feedback.json', 'w'), indent=2)
"

# Generate revision plan
python .claude/skills/lesson-designer/scripts/generate_revision_plan.py \
    test_feedback.json \
    test_revision_plan.json \
    --markdown test_revision_plan.md

# Verify outputs
python -c "
import json
plan = json.load(open('test_revision_plan.json'))
print('Critical changes:', len(plan['critical_changes']))
print('Optional improvements:', len(plan['optional_improvements']))
assert len(plan['critical_changes']) == 2  # vocabulary and scaffolding
assert len(plan['optional_improvements']) == 1  # pacing
"

cat test_revision_plan.md | head -50

# Cleanup
rm test_feedback.json test_revision_plan.json test_revision_plan.md
```

Expected: Revision plan JSON with 2 critical changes (vocabulary, scaffolding) and 1 optional improvement (pacing). Markdown shows teacher-readable format with approve/reject checkboxes.
</verification>

<success_criteria>
- generate_revision_plan.py produces structured revision plan JSON
- Markdown output is teacher-readable with approve/reject options
- SKILL.md documents complete Stage 3.5 workflow
- Architecture supports Phase 4 scaling (aggregates N persona feedbacks)
- apply_revisions() function ready to apply teacher-approved changes
</success_criteria>

<output>
After completion, create `.planning/phases/03-single-persona-feedback/03-02-SUMMARY.md`
</output>
