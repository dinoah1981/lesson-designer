---
phase: 03-single-persona-feedback
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .claude/skills/lesson-designer/personas/struggling_learner.json
  - .claude/skills/lesson-designer/scripts/persona_evaluator.py
autonomous: true

must_haves:
  truths:
    - "Struggling learner persona is defined with concrete characteristics"
    - "Evaluator can assess a lesson design and produce structured feedback"
    - "Feedback includes strengths, concerns with severity, and recommendations"
  artifacts:
    - path: ".claude/skills/lesson-designer/personas/struggling_learner.json"
      provides: "Alex persona definition with reading level, vocabulary, attention, scaffolding needs"
      contains: "persona_id"
    - path: ".claude/skills/lesson-designer/scripts/persona_evaluator.py"
      provides: "Parameterized persona evaluator class"
      exports: ["PersonaEvaluator", "evaluate_lesson_with_persona"]
  key_links:
    - from: "persona_evaluator.py"
      to: "struggling_learner.json"
      via: "loads persona config"
      pattern: "json\\.load.*persona"
    - from: "persona_evaluator.py"
      to: "04_lesson_final.json"
      via: "reads lesson design"
      pattern: "json\\.load.*lesson"
---

<objective>
Create the struggling learner persona definition and parameterized evaluator script that can assess any lesson design.

Purpose: Establish the foundation for persona-based feedback. The persona definition provides concrete characteristics (reading level 2-3 years below grade, tier 2/3 vocabulary barriers, 15-20 min attention span, scaffolding needs). The evaluator uses these characteristics to analyze lessons and produce structured feedback.

Output:
- personas/struggling_learner.json - Complete persona definition for "Alex"
- scripts/persona_evaluator.py - Evaluator that works with any persona JSON
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-single-persona-feedback/03-RESEARCH.md

# Existing lesson design structure
@.claude/skills/lesson-designer/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create personas directory and struggling learner definition</name>
  <files>
    .claude/skills/lesson-designer/personas/struggling_learner.json
  </files>
  <action>
Create the personas directory and struggling_learner.json with the complete "Alex" persona definition from research.

Include these fields:
- persona_id: "struggling_learner_ell"
- persona_name: "Alex"
- description: "8th grade student reading 2-3 years below grade level with limited academic vocabulary"
- characteristics:
  - reading_level: { grade_equivalent: "5-6", actual_grade: "8", gap_years: 2, fluency_issues: true, vocabulary_gaps: true }
  - vocabulary: { tier_1_familiar: true, tier_2_academic: false, tier_3_domain: false, needs_explicit_definitions: true, needs_visual_supports: true }
  - attention: { sustained_focus_minutes: 20, multistep_limit: 3, needs_breaks: true }
  - background_knowledge: { assumes_limited_schema: true, needs_cultural_context: true, requires_explicit_connections: true }
  - scaffolding: { needs_models: true, needs_sentence_frames: true, needs_graphic_organizers: true, needs_worked_examples: true }
- evaluation_criteria: ["vocabulary_accessibility", "instruction_clarity", "scaffolding_adequacy", "pacing_appropriateness", "engagement_accessibility"]
- decision_rules: (document the specific thresholds from research)
  - vocabulary: "Flag tier 2/3 terms without explicit definition or visual support"
  - pacing: "Flag activities longer than 20 min without clear break points"
  - scaffolding: "Flag production tasks (writing, speaking) without models or sentence frames"
  - instructions: "Flag tasks with >3 steps without checklist or visual"

This parameterized format supports Phase 4 scaling - same structure for additional personas.
  </action>
  <verify>
python -c "import json; p = json.load(open('.claude/skills/lesson-designer/personas/struggling_learner.json')); assert 'persona_id' in p and 'evaluation_criteria' in p and len(p['evaluation_criteria']) == 5; print('Persona definition valid')"
  </verify>
  <done>
- personas/struggling_learner.json exists
- Contains all 5 characteristic categories (reading_level, vocabulary, attention, background_knowledge, scaffolding)
- Contains all 5 evaluation criteria
- Contains decision_rules for automated evaluation
  </done>
</task>

<task type="auto">
  <name>Task 2: Create parameterized persona evaluator script</name>
  <files>
    .claude/skills/lesson-designer/scripts/persona_evaluator.py
  </files>
  <action>
Create persona_evaluator.py with a PersonaEvaluator class that can evaluate any lesson using any persona definition.

Structure:
```python
#!/usr/bin/env python3
"""
Persona-based lesson evaluator for accessibility feedback.

Usage:
    python persona_evaluator.py <lesson_json> <persona_json> <output_json>

Example:
    python persona_evaluator.py \
        .lesson-designer/sessions/{session_id}/04_lesson_final.json \
        .claude/skills/lesson-designer/personas/struggling_learner.json \
        .lesson-designer/sessions/{session_id}/03_feedback_struggling_learner.json
"""

import json
import sys
import argparse
from datetime import datetime
from pathlib import Path

class PersonaEvaluator:
    def __init__(self, persona_config):
        self.persona = persona_config
        # Map evaluation criteria to methods
        self.evaluators = {
            'vocabulary_accessibility': self.evaluate_vocabulary,
            'instruction_clarity': self.evaluate_instructions,
            'scaffolding_adequacy': self.evaluate_scaffolding,
            'pacing_appropriateness': self.evaluate_pacing,
            'engagement_accessibility': self.evaluate_engagement
        }

    def evaluate_lesson(self, lesson):
        """Run all evaluation criteria from persona config."""
        # Returns structured feedback JSON

    def evaluate_vocabulary(self, lesson):
        """Check vocabulary against persona reading level and needs."""
        # Flag tier 2/3 terms without definitions
        # Flag undefined academic vocabulary
        # Note strengths (if definitions provided)

    def evaluate_instructions(self, lesson):
        """Check instruction clarity against persona limits."""
        # Flag instructions with >3 steps without checklist
        # Flag vague success criteria
        # Check sentence complexity (avg >20 words = flag)

    def evaluate_scaffolding(self, lesson):
        """Check scaffolding against persona needs."""
        # Flag writing tasks without models/sentence frames
        # Flag higher-order activities without visual supports
        # Flag lack of worked examples for complex tasks

    def evaluate_pacing(self, lesson):
        """Check activity pacing against attention span."""
        # Flag activities >20 min without breaks
        # Flag consecutive passive activities >15 min
        # Check for break points in longer activities

    def evaluate_engagement(self, lesson):
        """Check engagement accessibility."""
        # Check for multiple entry points (visual, verbal, kinesthetic)
        # Check for student choice opportunities
        # Note strengths in activity variety

    def calculate_rating(self, concerns):
        """Calculate 1-5 accessibility rating based on concern count/severity."""
        # 5: No high concerns, <=1 medium
        # 4: No high concerns, 2-3 medium
        # 3: 1 high concern OR 4+ medium
        # 2: 2-3 high concerns
        # 1: 4+ high concerns
```

Output feedback JSON structure:
- persona, persona_name, evaluation_date
- overall_assessment: { accessibility_rating: "N/5", summary: str, primary_concern: str }
- strengths: [ { element, observation, why_helpful } ]
- concerns: [ { element, issue, severity, impact, evidence, recommendation: { change, rationale, implementation } } ]
- pedagogical_notes: { overall_difficulty, priority_revisions, deferred_to_teacher }
- metadata: { confidence, persona_version }

Use persona decision_rules for automated detection. Severity levels: high, medium, low.
  </action>
  <verify>
python -c "
import sys
sys.path.insert(0, '.claude/skills/lesson-designer/scripts')
from persona_evaluator import PersonaEvaluator
import json

# Test with minimal lesson structure
lesson = {
    'title': 'Test Lesson',
    'vocabulary': [{'word': 'analyze', 'definition': None}],
    'activities': [
        {'name': 'Test Activity', 'duration': 25, 'marzano_level': 'analysis', 'instructions': ['Step 1', 'Step 2', 'Step 3', 'Step 4'], 'student_output': 'written response'}
    ]
}
persona = json.load(open('.claude/skills/lesson-designer/personas/struggling_learner.json'))
evaluator = PersonaEvaluator(persona)
feedback = evaluator.evaluate_lesson(lesson)
assert 'concerns' in feedback and 'overall_assessment' in feedback
print('Evaluator works - found', len(feedback['concerns']), 'concerns')
"
  </verify>
  <done>
- persona_evaluator.py exists with PersonaEvaluator class
- Evaluator loads any persona JSON and applies its evaluation criteria
- Produces structured feedback with strengths, concerns (severity-rated), recommendations
- Test shows evaluator detecting issues (vocabulary without definition, long activity, writing without scaffold)
  </done>
</task>

</tasks>

<verification>
Run full evaluation test with a sample lesson:

```bash
# Create test lesson file
python -c "
import json
lesson = {
    'title': 'Primary Source Analysis',
    'grade_level': '8th grade',
    'duration': 50,
    'vocabulary': [
        {'word': 'bias', 'definition': None},
        {'word': 'primary source', 'definition': 'An original document from the time period'}
    ],
    'activities': [
        {
            'name': 'Vocabulary Introduction',
            'duration': 5,
            'marzano_level': 'retrieval',
            'instructions': ['Review vocabulary terms'],
            'student_output': 'notes',
            'materials': ['vocabulary handout']
        },
        {
            'name': 'Document Analysis',
            'duration': 25,
            'marzano_level': 'analysis',
            'instructions': ['Read the primary source', 'Complete SOAP analysis', 'Write a 2-paragraph response', 'Discuss with partner'],
            'student_output': 'written response',
            'materials': ['Civil War letter', 'SOAP worksheet']
        }
    ],
    'assessment': {'type': 'exit_ticket', 'questions': ['Why is source reliability important?']}
}
json.dump(lesson, open('test_lesson.json', 'w'), indent=2)
"

# Run evaluator
python .claude/skills/lesson-designer/scripts/persona_evaluator.py \
    test_lesson.json \
    .claude/skills/lesson-designer/personas/struggling_learner.json \
    test_feedback.json

# Verify output
python -c "
import json
fb = json.load(open('test_feedback.json'))
print('Rating:', fb['overall_assessment']['accessibility_rating'])
print('Concerns:', len(fb['concerns']))
for c in fb['concerns']:
    print(f'  - [{c[\"severity\"]}] {c[\"element\"]}: {c[\"issue\"][:60]}...')
"

# Cleanup
rm test_lesson.json test_feedback.json
```

Expected: Evaluator flags vocabulary without definition, long activity without breaks, writing task without sentence frames.
</verification>

<success_criteria>
- Persona definition JSON loads and validates with all required fields
- PersonaEvaluator class instantiates with any persona JSON
- evaluate_lesson() produces structured feedback matching research schema
- Feedback includes actionable recommendations with rationale
- Architecture is parameterized (works for any persona definition)
</success_criteria>

<output>
After completion, create `.planning/phases/03-single-persona-feedback/03-01-SUMMARY.md`
</output>
