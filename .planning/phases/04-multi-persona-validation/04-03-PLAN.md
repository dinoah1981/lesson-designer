---
phase: 04-multi-persona-validation
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - .claude/skills/lesson-designer/scripts/generate_revision_plan.py
  - .claude/skills/lesson-designer/SKILL.md
autonomous: false

must_haves:
  truths:
    - "Tool synthesizes feedback from all personas into single revision plan"
    - "Conflicting recommendations are detected and presented with resolution strategies"
    - "Agreements across 3+ personas are elevated as universal improvements"
    - "Teacher receives categorized revision proposal (universal, accessibility, engagement, challenge, conflicts)"
    - "SKILL.md documents Stage 3.5 multi-persona workflow"
  artifacts:
    - path: ".claude/skills/lesson-designer/scripts/generate_revision_plan.py"
      provides: "Multi-persona synthesis with conflict detection"
      exports: ["detect_conflicts", "find_agreements", "synthesize_feedback"]
    - path: ".claude/skills/lesson-designer/SKILL.md"
      provides: "Stage 3.5 multi-persona workflow documentation"
      contains: "run_multi_persona"
  key_links:
    - from: "generate_revision_plan.py"
      to: "aggregate_feedback"
      via: "synthesize_feedback calls aggregate_feedback"
      pattern: "synthesize_feedback.*aggregate_feedback"
    - from: "generate_revision_plan.py"
      to: "detect_conflicts"
      via: "conflict detection in synthesis"
      pattern: "detect_conflicts.*scaffolding_vs_challenge"
---

<objective>
Add conflict detection and multi-persona synthesis to generate_revision_plan.py, then update SKILL.md and verify end-to-end workflow.

Purpose: Handle conflicting recommendations across personas using "add, don't subtract" strategy and present synthesized feedback to teachers.
Output: Enhanced revision plan generator with conflict handling + updated documentation.
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-multi-persona-validation/04-RESEARCH.md
@.claude/skills/lesson-designer/scripts/generate_revision_plan.py
@.claude/skills/lesson-designer/scripts/run_multi_persona.py
@.claude/skills/lesson-designer/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add conflict detection to generate_revision_plan.py</name>
  <files>.claude/skills/lesson-designer/scripts/generate_revision_plan.py</files>
  <action>
Add three new functions after existing aggregate_feedback() function:

**1. detect_conflicts(all_concerns: List[Dict]) -> List[Dict]:**
Identify conflicting recommendations across personas.

```python
def detect_conflicts(all_concerns: List[Dict]) -> List[Dict]:
    """
    Identify conflicting recommendations across personas.

    Conflicts occur when:
    - Same element gets opposite recommendations (add scaffolding vs reduce scaffolding)
    - Persona needs are inherently opposing (struggling needs support, high-achieving needs less)

    Returns:
        List of conflict objects with resolution strategies
    """
    conflicts = []

    # Group concerns by element
    by_element = {}
    for concern in all_concerns:
        element = concern['element']
        if element not in by_element:
            by_element[element] = []
        by_element[element].append(concern)

    # Check each element for conflicts
    for element, concerns in by_element.items():
        if len(concerns) < 2:
            continue

        # Check for scaffolding vs. challenge conflict
        struggling_concerns = [c for c in concerns if c.get('persona_id') == 'struggling_learner_ell']
        high_achieving_concerns = [c for c in concerns if c.get('persona_id') == 'high_achieving']

        has_add_scaffold = any(
            'add' in c.get('recommendation', {}).get('change', '').lower() and
            ('scaffold' in c.get('recommendation', {}).get('change', '').lower() or
             'support' in c.get('recommendation', {}).get('change', '').lower())
            for c in struggling_concerns
        )

        has_reduce_complexity = any(
            any(word in c.get('recommendation', {}).get('change', '').lower()
                for word in ['reduce', 'remove', 'simplify', 'less'])
            for c in high_achieving_concerns
        )

        if has_add_scaffold and has_reduce_complexity:
            conflicts.append({
                'element': element,
                'type': 'scaffolding_vs_challenge',
                'personas_involved': ['struggling_learner_ell', 'high_achieving'],
                'struggling_recommendation': struggling_concerns[0] if struggling_concerns else None,
                'high_achieving_recommendation': high_achieving_concerns[0] if high_achieving_concerns else None,
                'resolution_strategy': 'tiered_support',
                'teacher_note': 'Provide scaffolded version for struggling learners, challenge version for advanced students. Both groups maintain same learning objective.'
            })

    return conflicts
```

**2. find_agreements(all_concerns: List[Dict], threshold: int = 3) -> List[Dict]:**
Find recommendations where 3+ personas agree.

```python
def find_agreements(all_concerns: List[Dict], threshold: int = 3) -> List[Dict]:
    """
    Find recommendations where multiple personas agree.

    Args:
        all_concerns: All concerns from all personas
        threshold: Minimum number of personas that must agree (default: 3 of 4)

    Returns:
        List of concerns that are universally supported
    """
    # Group by element and similar recommendation patterns
    similar_groups = {}

    for concern in all_concerns:
        element = concern['element']
        change_text = concern.get('recommendation', {}).get('change', '')

        # Create grouping key from element and recommendation type (first 50 chars)
        key = f"{element}:{change_text[:50]}"

        if key not in similar_groups:
            similar_groups[key] = []
        similar_groups[key].append(concern)

    # Find groups meeting threshold
    agreements = []
    for key, group in similar_groups.items():
        persona_ids = set(c.get('persona_id', '') for c in group)
        if len(persona_ids) >= threshold:
            # Create synthesized concern
            agreements.append({
                'element': group[0]['element'],
                'severity': max(c.get('severity', 'medium') for c in group,
                               key=lambda s: {'high': 3, 'medium': 2, 'low': 1}.get(s, 0)),
                'issue': group[0]['issue'],
                'recommendation': group[0]['recommendation'],
                'personas_agreeing': [c.get('persona_name', '') for c in group],
                'agreement_count': len(persona_ids),
                'priority': 'universal'
            })

    return agreements
```

**3. synthesize_feedback(all_concerns: List[Dict]) -> Dict[str, Any]:**
Combine all concerns into categorized synthesis.

```python
def synthesize_feedback(all_concerns: List[Dict]) -> Dict[str, Any]:
    """
    Combine concerns from all personas into categorized revision plan.

    Categories:
    - universal_improvements: 3+ personas agree (highest priority)
    - accessibility_critical: Struggling learner high severity concerns
    - engagement_enhancements: Unmotivated/interested persona recommendations
    - challenge_extensions: High-achieving persona recommendations
    - conflicting_recommendations: Require teacher decision with resolution strategies

    Returns:
        Categorized feedback dictionary
    """
    universal = find_agreements(all_concerns, threshold=3)
    conflicts = detect_conflicts(all_concerns)

    # Filter by persona for non-universal concerns
    def filter_by_persona(concerns, persona_ids, severity=None):
        if isinstance(persona_ids, str):
            persona_ids = [persona_ids]
        filtered = [c for c in concerns if c.get('persona_id') in persona_ids]
        if severity:
            filtered = [c for c in filtered if c.get('severity') == severity]
        return filtered

    # Get concerns already handled by universal or conflicts
    universal_elements = set(c['element'] for c in universal)
    conflict_elements = set(c['element'] for c in conflicts)
    handled = universal_elements | conflict_elements

    # Filter remaining concerns
    remaining = [c for c in all_concerns if c['element'] not in handled]

    return {
        'universal_improvements': universal,
        'accessibility_critical': filter_by_persona(remaining, 'struggling_learner_ell', 'high'),
        'engagement_enhancements': filter_by_persona(remaining, ['unmotivated_capable', 'interested_capable']),
        'challenge_extensions': filter_by_persona(remaining, 'high_achieving'),
        'conflicting_recommendations': conflicts,
        'metadata': {
            'total_concerns': len(all_concerns),
            'universal_count': len(universal),
            'conflicts_count': len(conflicts),
            'personas_analyzed': list(set(c.get('persona_id', '') for c in all_concerns))
        }
    }
```

**Also update generate_revision_plan() function:**
- Call synthesize_feedback() when multiple personas consulted
- Add new section in revision plan: 'synthesis' containing the categorized feedback
- Update render_revision_markdown() to include synthesis sections
  </action>
  <verify>
python -c "import sys; sys.path.insert(0, '.claude/skills/lesson-designer/scripts'); from generate_revision_plan import detect_conflicts, find_agreements, synthesize_feedback; print('All functions imported OK')"
  </verify>
  <done>Three new functions (detect_conflicts, find_agreements, synthesize_feedback) are importable and have correct signatures</done>
</task>

<task type="auto">
  <name>Task 2: Update SKILL.md with Stage 3.5 multi-persona workflow</name>
  <files>.claude/skills/lesson-designer/SKILL.md</files>
  <action>
Update SKILL.md Stage 3.5 section to document multi-persona workflow.

**Locate Stage 3.5 section and update to:**

```markdown
### Stage 3.5: Multi-Persona Feedback (Optional)

Run the lesson design through 4 diverse student personas to identify accessibility barriers, engagement issues, and ceiling limitations.

**Personas:**
1. **Alex (Struggling/ELL)** - Reading 2-3 years below grade level, vocabulary gaps
2. **Jordan (Unmotivated Capable)** - High ability, low engagement, needs relevance
3. **Maya (Interested Capable)** - High ability, high engagement, wants depth
4. **Marcus (High Achieving)** - Gifted learner, rapid mastery, needs challenge

**Step 1: Run Multi-Persona Evaluation**

```bash
python .claude/skills/lesson-designer/scripts/run_multi_persona.py \
    .lesson-designer/sessions/{session_id}/04_lesson_final.json \
    .lesson-designer/sessions/{session_id}/
```

Output: 4 feedback files
- `03_feedback_struggling_learner_ell.json`
- `03_feedback_unmotivated_capable.json`
- `03_feedback_interested_capable.json`
- `03_feedback_high_achieving.json`

**Step 2: Generate Synthesized Revision Plan**

```bash
python .claude/skills/lesson-designer/scripts/generate_revision_plan.py \
    "03_feedback_struggling_learner_ell.json,03_feedback_unmotivated_capable.json,03_feedback_interested_capable.json,03_feedback_high_achieving.json" \
    03_revision_plan.json \
    --lesson 04_lesson_final.json \
    --markdown 03_revision_plan.md
```

Output: Revision plan with synthesis categories:
- **Universal Improvements** - 3+ personas agree (highest priority)
- **Accessibility Critical** - Struggling learner barriers
- **Engagement Enhancements** - Motivation/interest suggestions
- **Challenge Extensions** - High-achieving needs
- **Conflicting Recommendations** - Require teacher decision

**Step 3: Teacher Review**

Present `03_revision_plan.md` to teacher for approval.

Conflict resolution strategies:
- **Tiered Support**: Scaffolded version for struggling + challenge version for advanced
- **Core + Extension**: All students do core, capable students extend
- **Optionality**: Multiple paths to same learning goal

**Step 4: Apply Revisions**

```bash
python .claude/skills/lesson-designer/scripts/generate_revision_plan.py \
    --apply \
    --lesson 04_lesson_final.json \
    --revision-plan 03_revision_plan.json \
    --output 04_lesson_revised.json
```

**Differentiation Principle:** Add, don't subtract. Never remove scaffolding to add challenge or simplify to add accessibility. Use tiering and optionality instead.
```
  </action>
  <verify>grep -c "run_multi_persona" .claude/skills/lesson-designer/SKILL.md</verify>
  <done>SKILL.md contains "run_multi_persona" at least once, Stage 3.5 section documents 4 personas and synthesis workflow</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Multi-persona validation system with:
- 3 new persona definitions (Jordan, Maya, Marcus)
- run_multi_persona.py orchestrator
- Conflict detection and synthesis in generate_revision_plan.py
- Updated SKILL.md documentation
  </what-built>
  <how-to-verify>
1. Run test lesson through all 4 personas:
   ```bash
   cd "C:\Users\david\OneDrive\Desktop\lesson-designer"
   python .claude/skills/lesson-designer/scripts/run_multi_persona.py \
       test-files/sample_lesson.json \
       test-files/feedback/
   ```

2. Verify 4 feedback files created in test-files/feedback/

3. Generate synthesized revision plan:
   ```bash
   python .claude/skills/lesson-designer/scripts/generate_revision_plan.py \
       "test-files/feedback/03_feedback_struggling_learner_ell.json,test-files/feedback/03_feedback_unmotivated_capable.json,test-files/feedback/03_feedback_interested_capable.json,test-files/feedback/03_feedback_high_achieving.json" \
       test-files/03_revision_plan.json \
       --markdown test-files/03_revision_plan.md
   ```

4. Open test-files/03_revision_plan.md and verify:
   - Universal improvements section exists
   - Conflicts section exists with resolution strategies
   - Changes are categorized by persona type
   - Teacher decision checkboxes present

5. Review SKILL.md Stage 3.5 section for completeness
  </how-to-verify>
  <resume-signal>Type "approved" if multi-persona workflow produces synthesized feedback, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
1. detect_conflicts() identifies scaffolding vs challenge conflicts
2. find_agreements() returns concerns where 3+ personas agree
3. synthesize_feedback() categorizes all concerns correctly
4. generate_revision_plan.py produces categorized output with synthesis section
5. render_revision_markdown() includes all synthesis categories
6. SKILL.md Stage 3.5 documents multi-persona workflow with all 4 personas
7. End-to-end test produces expected outputs
</verification>

<success_criteria>
- Conflict detection identifies opposing recommendations between struggling + high-achieving
- Agreement detection finds recommendations with 3+ persona support
- Synthesized revision plan has 5 categories: universal, accessibility, engagement, challenge, conflicts
- Teacher receives clear revision proposal with resolution strategies for conflicts
- SKILL.md documents complete Stage 3.5 multi-persona workflow
- Human verification confirms end-to-end workflow works
</success_criteria>

<output>
After completion, create `.planning/phases/04-multi-persona-validation/04-03-SUMMARY.md`
</output>
