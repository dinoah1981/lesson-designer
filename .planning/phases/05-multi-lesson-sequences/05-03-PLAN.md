---
phase: 05-multi-lesson-sequences
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - .claude/skills/lesson-designer/scripts/generate_sequence_assessment.py
  - .claude/skills/lesson-designer/tests/test_sequence_assessment.py
autonomous: true

must_haves:
  truths:
    - "Tool can generate sequence-level assessments covering multiple lessons"
    - "Assessment draws on vocabulary, skills, and knowledge from entire sequence"
    - "Summative assessment tests integration of competencies, not just individual skills"
  artifacts:
    - path: ".claude/skills/lesson-designer/scripts/generate_sequence_assessment.py"
      provides: "Sequence-level assessment generation"
      exports: ["generate_sequence_assessment", "SequenceAssessmentConfig"]
  key_links:
    - from: "generate_sequence_assessment.py"
      to: "sequence_manager.py"
      via: "imports sequence metadata"
      pattern: "from sequence_manager import"
    - from: "generate_sequence_assessment.py"
      to: "generate_assessment.py"
      via: "extends existing assessment patterns"
      pattern: "rubric structure, answer key format"
---

<objective>
Create sequence-level assessment generation that covers multiple lessons.

Purpose: Teachers need summative assessments (final tests, performance tasks) that evaluate student mastery across an entire 2-4 week unit, not just individual lesson objectives. Uses backward design - assessment measures end-of-sequence competency.

Output: New generate_sequence_assessment.py that creates comprehensive assessments drawing from all lessons in a sequence.
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-multi-lesson-sequences/05-RESEARCH.md
@.planning/phases/05-multi-lesson-sequences/05-01-SUMMARY.md

# Existing assessment generator to extend
@.claude/skills/lesson-designer/scripts/generate_assessment.py

# Sequence manager for metadata access
@.claude/skills/lesson-designer/scripts/sequence_manager.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create generate_sequence_assessment.py</name>
  <files>.claude/skills/lesson-designer/scripts/generate_sequence_assessment.py</files>
  <action>
Create sequence-level assessment generator.

Implement:

1. `SequenceAssessmentConfig` dataclass:
   ```python
   @dataclass
   class SequenceAssessmentConfig:
       assessment_type: str  # "cumulative_test", "performance_task", "portfolio_review"
       title: str
       time_limit: int = 60  # minutes
       include_lessons: list[int] = None  # None = all lessons
       emphasis_competencies: list[str] = None  # Competency IDs to emphasize
   ```

2. `generate_sequence_assessment(sequence_id: str, config: SequenceAssessmentConfig) -> dict`
   - Load sequence_metadata.json
   - Load all lesson_summary.json files (or subset per config.include_lessons)
   - Build comprehensive assessment:

   For `cumulative_test`:
   - Section per competency with questions at varied Marzano levels
   - Questions reference vocabulary from vocabulary_progression
   - Include retrieval (terminology), comprehension (explain), analysis (compare/evaluate), knowledge_utilization (apply to new scenario)
   - Output: JSON + .docx test with answer key

   For `performance_task`:
   - Single complex task requiring integration of all competencies
   - Rubric with criteria from each competency
   - Uses backward design: task demonstrates end-of-sequence proficiency
   - Output: JSON + .docx task description + rubric

   For `portfolio_review`:
   - Reflection prompts for each lesson's key learning
   - Self-assessment rubric
   - Output: JSON + .docx portfolio guide

   Return assessment dict matching existing assessment schema (see generate_assessment.py)

3. `_build_cumulative_test(metadata: dict, summaries: list[dict], config: SequenceAssessmentConfig) -> dict`
   - Create test structure with sections
   - Each section has questions at different Marzano levels
   - Include vocabulary terms as retrieval questions
   - Include synthesis questions requiring multiple skills

4. `_build_performance_task(metadata: dict, summaries: list[dict], config: SequenceAssessmentConfig) -> dict`
   - Create authentic task requiring competency integration
   - Build rubric with criteria from each competency
   - Include success criteria aligned to proficiency targets

5. `_create_assessment_docx(assessment: dict, output_dir: Path) -> Path`
   - Generate Word document for the assessment
   - Follow formatting from generate_assessment.py (double-spacing, answer lines)
   - Create answer key for cumulative_test type

Import from sequence_manager: get_sequence_metadata, get_lesson_directory
Import from generate_assessment: formatting utilities if available

Follow existing code style with docstrings and type hints.
  </action>
  <verify>
Run: `python -c "from generate_sequence_assessment import generate_sequence_assessment, SequenceAssessmentConfig; print('Import OK')"`
Expected: "Import OK" with no errors
  </verify>
  <done>
generate_sequence_assessment.py exists with all functions, imports work correctly
  </done>
</task>

<task type="auto">
  <name>Task 2: Create tests for sequence assessment generation</name>
  <files>.claude/skills/lesson-designer/tests/test_sequence_assessment.py</files>
  <action>
Create test file for sequence assessment generator.

Test cases:

1. `test_cumulative_test_generation`
   - Create sequence with 3 lessons and summaries
   - Generate cumulative_test assessment
   - Verify:
     - Assessment has sections for each competency
     - Questions span Marzano levels (retrieval through knowledge_utilization)
     - Vocabulary from vocabulary_progression appears in questions
     - Answer key is generated

2. `test_performance_task_generation`
   - Create sequence with 2 competencies
   - Generate performance_task assessment
   - Verify:
     - Task description references both competencies
     - Rubric has criteria from each competency
     - Success criteria align to proficiency targets

3. `test_partial_lesson_assessment`
   - Create sequence with 5 lessons
   - Generate assessment for lessons 1-3 only (mid-unit check)
   - Verify:
     - Only lessons 1-3 vocabulary/skills included
     - Lessons 4-5 content not referenced

4. `test_assessment_docx_creation`
   - Generate cumulative_test
   - Verify:
     - .docx file created in output directory
     - File is valid Word document (opens without error)
     - Contains expected sections

Use pytest fixtures for sequence/lesson setup.
Create minimal mock lesson summaries with vocabulary and objectives.
  </action>
  <verify>
Run: `pytest .claude/skills/lesson-designer/tests/test_sequence_assessment.py -v`
Expected: All 4 tests pass
  </verify>
  <done>
test_sequence_assessment.py exists, all tests pass, validates sequence assessment generation works
  </done>
</task>

</tasks>

<verification>
1. `python -c "from generate_sequence_assessment import *; print('OK')"` - imports work
2. `pytest .claude/skills/lesson-designer/tests/test_sequence_assessment.py -v` - all tests pass
3. Cumulative test includes questions from all lessons
4. Performance task integrates multiple competencies
5. Generated .docx files are valid Word documents
</verification>

<success_criteria>
- Tool can generate cumulative tests covering vocabulary and skills from entire sequence
- Tool can generate performance tasks requiring competency integration
- Assessments use backward design aligned to end-of-sequence proficiency
- Generated documents follow existing formatting standards
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-lesson-sequences/05-03-SUMMARY.md`
</output>
