---
phase: 05-multi-lesson-sequences
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - .claude/skills/lesson-designer/scripts/sequence_context.py
  - .claude/skills/lesson-designer/tests/test_sequence_context.py
autonomous: true

must_haves:
  truths:
    - "Lesson summaries are created after each lesson generation"
    - "Context for lesson N includes prior lesson summaries"
    - "Vocabulary already taught is tracked and provided to lesson design"
    - "Tool knows what came before when designing next lesson"
  artifacts:
    - path: ".claude/skills/lesson-designer/scripts/sequence_context.py"
      provides: "Context assembly and lesson summarization"
      exports: ["create_lesson_summary", "build_context_for_lesson", "check_vocabulary_continuity", "update_vocabulary_progression"]
  key_links:
    - from: "sequence_context.py"
      to: "sequence_manager.py"
      via: "imports sequence metadata functions"
      pattern: "from sequence_manager import"
    - from: "sequence_context.py"
      to: "lesson_summary.json"
      via: "creates summary after lesson generation"
      pattern: "lesson_summary.json"
---

<objective>
Implement context awareness across lessons with summarization and vocabulary tracking.

Purpose: When designing lesson 4, the tool should know what was covered in lessons 1-3 (vocabulary, skills, concerns from persona feedback). This prevents quality degradation and maintains progression coherence.

Output: New sequence_context.py with lesson summarization, context assembly, and vocabulary continuity checking.
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-multi-lesson-sequences/05-RESEARCH.md
@.planning/phases/05-multi-lesson-sequences/05-01-SUMMARY.md

# Sequence manager created in prior plan
@.claude/skills/lesson-designer/scripts/sequence_manager.py

# Lesson JSON structure reference
@.claude/skills/lesson-designer/scripts/design_lesson.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sequence_context.py with summarization and context assembly</name>
  <files>.claude/skills/lesson-designer/scripts/sequence_context.py</files>
  <action>
Create sequence_context.py with context management functions.

Implement these functions:

1. `create_lesson_summary(sequence_id: str, lesson_num: int, lesson_json: dict, persona_feedback: list[dict] = None) -> dict`
   - Extract compressed summary from 04_lesson_final.json
   - Summary schema (target ~250 tokens):
     ```json
     {
       "lesson_number": 1,
       "title": "Lesson title",
       "objective": "Learning objective",
       "lesson_type": "introducing_skill",
       "vocabulary_introduced": [
         {"term": "primary source", "definition": "...", "example": "..."}
       ],
       "assumed_knowledge": ["list of assumed items"],
       "marzano_distribution": {"retrieval": 1, "comprehension": 2, "analysis": 1, "knowledge_utilization": 1},
       "cognitive_rigor_percent": 40,
       "pedagogical_notes": {
         "concerns": ["high severity concerns from personas"],
         "successes": ["positive feedback"]
       },
       "duration": 55,
       "token_estimate": 250
     }
     ```
   - Save to lesson_summary.json in lesson directory
   - Return summary dict

2. `build_context_for_lesson(sequence_id: str, lesson_num: int) -> dict`
   - Assemble context package for designing lesson N
   - Include:
     - sequence_metadata (always)
     - current_competency (from metadata based on lesson_range)
     - prior_lessons: list of lesson_summary.json from lessons 1 to N-1
     - vocabulary_already_taught: accumulated set from vocabulary_progression
     - vocabulary_to_introduce: from metadata for this lesson
   - For 2-4 lesson sequences, include ALL prior summaries directly (research shows JSON sufficient)
   - Return context dict

3. `check_vocabulary_continuity(sequence_id: str, lesson_num: int, draft_lesson: dict) -> dict`
   - Validate vocabulary usage for coherence
   - Return:
     ```json
     {
       "previously_taught": ["terms this lesson can use without defining"],
       "newly_introduced": ["terms this lesson introduces"],
       "incorrectly_assumed": ["terms used but never taught - ERROR"],
       "is_coherent": true
     }
     ```
   - Extract used terms from draft_lesson activities text

4. `update_vocabulary_progression(sequence_id: str, lesson_num: int, terms: list[str]) -> None`
   - Add terms to vocabulary_progression[lesson_XX] in sequence metadata
   - Update via save_sequence_metadata

5. `calculate_higher_order_percent(lesson_json: dict) -> int`
   - Count activities at analysis or knowledge_utilization level
   - Return percentage as integer (0-100)

Helper function:
- `extract_vocabulary_from_lesson(lesson_json: dict) -> list[str]`
  - Extract all vocabulary terms mentioned in activities
  - Look for "vocabulary" fields in activities, plus key terms in instructions

Import from sequence_manager: get_sequence_metadata, save_sequence_metadata, get_lesson_directory

Include comprehensive docstrings and type hints.
  </action>
  <verify>
Run: `python -c "from sequence_context import *; print('Import OK')"`
Expected: "Import OK" with no errors
  </verify>
  <done>
sequence_context.py exists with all 5 functions, imports work correctly
  </done>
</task>

<task type="auto">
  <name>Task 2: Create tests for context management</name>
  <files>.claude/skills/lesson-designer/tests/test_sequence_context.py</files>
  <action>
Create test file for sequence_context.py functions.

Test cases:

1. `test_create_lesson_summary`
   - Create sequence, add mock lesson JSON
   - Create summary, verify:
     - lesson_summary.json created in lesson directory
     - Summary contains required fields (vocabulary_introduced, marzano_distribution, etc.)
     - token_estimate is reasonable (~200-400)

2. `test_build_context_for_lesson`
   - Create sequence with 3 lessons
   - Create summaries for lessons 1 and 2
   - Build context for lesson 3, verify:
     - prior_lessons contains summaries for 1 and 2
     - vocabulary_already_taught includes terms from 1 and 2
     - current_competency is set correctly

3. `test_check_vocabulary_continuity_valid`
   - Set vocabulary_progression for lessons 1-2
   - Check lesson 3 draft that uses those terms
   - Verify: is_coherent = true, no incorrectly_assumed

4. `test_check_vocabulary_continuity_invalid`
   - Set vocabulary_progression for lesson 1 only
   - Check lesson 2 draft that uses undefined term
   - Verify: is_coherent = false, incorrectly_assumed contains the term

5. `test_update_vocabulary_progression`
   - Update vocabulary for lesson 2
   - Reload metadata, verify vocabulary_progression["lesson_02"] contains terms

Use pytest fixtures for sequence setup.
Create minimal mock lesson JSON with activities and vocabulary.
  </action>
  <verify>
Run: `pytest .claude/skills/lesson-designer/tests/test_sequence_context.py -v`
Expected: All 5 tests pass
  </verify>
  <done>
test_sequence_context.py exists, all tests pass, validates context management works correctly
  </done>
</task>

</tasks>

<verification>
1. `python -c "from sequence_context import create_lesson_summary, build_context_for_lesson; print('OK')"` - imports work
2. `pytest .claude/skills/lesson-designer/tests/test_sequence_context.py -v` - all tests pass
3. lesson_summary.json is created after calling create_lesson_summary()
4. build_context_for_lesson() returns prior lesson summaries
5. check_vocabulary_continuity() detects undefined terms
</verification>

<success_criteria>
- Lesson summaries are created with ~250 token compressed representation
- Context for lesson N includes all prior lesson summaries (for 2-4 lesson sequences)
- Vocabulary continuity is validated with clear error reporting
- Tool maintains context awareness across the sequence
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-lesson-sequences/05-02-SUMMARY.md`
</output>
